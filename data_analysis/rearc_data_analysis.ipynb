{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9ac12c-fb43-4d3f-af4c-9b2b65e67ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/02/17 02:17:32 WARN Utils: Your hostname, MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 172.20.10.3 instead (on interface en0)\n",
      "26/02/17 02:17:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/anantkhannekar/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/anantkhannekar/.ivy2.5.2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5b2afc46-5371-477b-80f5-e9734db729a9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 110ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5b2afc46-5371-477b-80f5-e9734db729a9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "26/02/17 02:17:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "  .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "            \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    " .config(\"spark.hadoop.fs.s3a.vectored.read.min.seek.size\", \"131072\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.vectored.read.max.merged.size\", \"2097152\") \\\n",
    "       .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "93063aaf-3681-4b58-b198-ac13c0f666f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- series_id: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- period: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- footnote_codes: string (nullable = true)\n",
      "\n",
      "+-----------------+----+------+-----+--------------+\n",
      "|        series_id|year|period|value|footnote_codes|\n",
      "+-----------------+----+------+-----+--------------+\n",
      "|PRS30006011      |1988|   Q01|  1.9|          NULL|\n",
      "|PRS30006011      |1988|   Q02|  2.2|          NULL|\n",
      "|PRS30006011      |1988|   Q03|  1.9|          NULL|\n",
      "|PRS30006011      |1988|   Q04|  1.1|          NULL|\n",
      "|PRS30006011      |1988|   Q05|  1.8|          NULL|\n",
      "+-----------------+----+------+-----+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_bls = spark.read.parquet(\"s3a://rearc-assignment-bls/raw/bls_data/part-0.parquet\")\n",
    "df_bls.printSchema()\n",
    "df_bls.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5480c14d-b0f3-4440-bdd9-c738db1d77c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim\n",
    "\n",
    "df_bls = (\n",
    "    df_bls\n",
    "    .withColumn(\"series_id\", trim(col(\"series_id\")))\n",
    "    .withColumn(\"year\", trim(col(\"year\")).cast(\"int\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "64330496-edc9-477c-b62b-ec51349b634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- annotations: struct (nullable = true)\n",
      " |    |-- dataset_link: string (nullable = true)\n",
      " |    |-- dataset_name: string (nullable = true)\n",
      " |    |-- source_description: string (nullable = true)\n",
      " |    |-- source_name: string (nullable = true)\n",
      " |    |-- subtopic: string (nullable = true)\n",
      " |    |-- table_id: string (nullable = true)\n",
      " |    |-- topic: string (nullable = true)\n",
      " |-- columns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Nation: string (nullable = true)\n",
      " |    |    |-- Nation ID: string (nullable = true)\n",
      " |    |    |-- Population: double (nullable = true)\n",
      " |    |    |-- Year: long (nullable = true)\n",
      " |-- page: struct (nullable = true)\n",
      " |    |-- limit: long (nullable = true)\n",
      " |    |-- offset: long (nullable = true)\n",
      " |    |-- total: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw_population  = spark.read.option(\"multiLine\", \"true\") \\\n",
    "    .json(\"s3a://rearc-assignment-bls/raw/datausa/population.json\")\n",
    "df_raw_population.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a8274cea-8f78-4fa7-a1d8-13103ba40988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----------+----+\n",
      "|       Nation|Nation ID|population|year|\n",
      "+-------------+---------+----------+----+\n",
      "|United States|  01000US| 316128839|2013|\n",
      "|United States|  01000US| 318857056|2014|\n",
      "|United States|  01000US| 321418821|2015|\n",
      "|United States|  01000US| 323127515|2016|\n",
      "|United States|  01000US| 325719178|2017|\n",
      "|United States|  01000US| 327167439|2018|\n",
      "|United States|  01000US| 328239523|2019|\n",
      "|United States|  01000US| 331893745|2021|\n",
      "|United States|  01000US| 333287562|2022|\n",
      "|United States|  01000US| 334914896|2023|\n",
      "+-------------+---------+----------+----+\n",
      "\n",
      "root\n",
      " |-- Nation: string (nullable = true)\n",
      " |-- Nation ID: string (nullable = true)\n",
      " |-- population: long (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_population = df_raw_population.select(explode(\"data\").alias(\"record\")) \\\n",
    "    .select(\"record.*\")\n",
    "\n",
    "df_population = (\n",
    "    df_population\n",
    "    .withColumn(\"year\", trim(col(\"Year\")).cast(\"int\"))\n",
    "    .withColumn(\n",
    "    \"population\",col(\"Population\").cast(\"long\")\n",
    ")\n",
    ")\n",
    "df_population.show()\n",
    "df_population.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "24aee7fa-468c-456d-a7f9-847b71721266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 322069808.0\n",
      "StdDev: 4158441.040908092\n"
     ]
    }
   ],
   "source": [
    "\"\"\"1. Using the dataframe from the population data API (Part 2), generate the mean and the standard deviation of the annual US population across the years [2013, 2018] inclusive.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import col, mean, stddev\n",
    "\n",
    "df_stats = (\n",
    "    df_population\n",
    "    .filter((col(\"Year\") >= 2013) & (col(\"Year\") <= 2018))\n",
    "    .agg(\n",
    "        mean(\"Population\").alias(\"mean_population\"),\n",
    "        stddev(\"Population\").alias(\"stddev_population\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# df_stats.show()\n",
    "\n",
    "result = df_stats.collect()[0]\n",
    "\n",
    "mean_population = result[\"mean_population\"]\n",
    "stddev_population = result[\"stddev_population\"]\n",
    "\n",
    "print(\"Mean:\", mean_population)\n",
    "print(\"StdDev:\", stddev_population)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8172c3f9-8c04-4f49-9855-da03ed008489",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2. Using the dataframe from the time-series (Part 1), For every series_id, find the best year: the year with the max/largest sum of \"value\" for all quarters in that year. Generate a report with each series id, the best year for that series, and the summed value for that year. For example, if the table had the following values:\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "df_year_data = (\n",
    "    df_bls\n",
    "    .groupBy(\"series_id\", \"year\")\n",
    "    .agg(_sum(\"value\").alias(\"year_total\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4a54565d-ac89-4f62-a870-2b9e770d6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc\n",
    "\n",
    "window_spec = Window.partitionBy(\"series_id\").orderBy(desc(\"year_total\"))\n",
    "\n",
    "df_ranked = (\n",
    "    df_year_data\n",
    "    .withColumn(\"rn\", row_number().over(window_spec))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e7070a8-c8ec-425c-b758-f4c3d3abafe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+------------+\n",
      "|        series_id|best_year|summed_value|\n",
      "+-----------------+---------+------------+\n",
      "|PRS30006033      |     2018|      508.86|\n",
      "|PRS30006063      |     1999|      411.95|\n",
      "|PRS30006063      |     2010|      403.22|\n",
      "|PRS30006081      |     1989|         6.4|\n",
      "|PRS30006113      |     2017|      499.99|\n",
      "|PRS30006132      |     1995|         4.5|\n",
      "|PRS30006152      |     2007|        -3.3|\n",
      "|PRS30006162      |     1998|        17.7|\n",
      "|PRS30006163      |     2001|       347.7|\n",
      "|PRS30006172      |     2011|        -6.8|\n",
      "|PRS30006213      |     1993|       351.4|\n",
      "|PRS31006022      |     2018|        -3.6|\n",
      "|PRS31006031      |     1988|        19.7|\n",
      "|PRS31006033      |     2003|      563.23|\n",
      "|PRS31006033      |     2012|      482.58|\n",
      "|PRS31006131      |     2003|         0.9|\n",
      "|PRS31006152      |     2023|        13.0|\n",
      "|PRS32006021      |     1995|        -2.5|\n",
      "|PRS32006022      |     2000|        -3.1|\n",
      "|PRS32006061      |     2009|       -29.6|\n",
      "+-----------------+---------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "\n",
    "df_best_year_data = df_ranked.select(\n",
    "    \"series_id\",\n",
    "    col(\"year\").alias(\"best_year\"),\n",
    "    round(col(\"year_total\").cast(\"double\"), 2).alias(\"summed_value\")\n",
    ")\n",
    "\n",
    "\n",
    "df_best_year_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "65570dac-ee14-46ef-a3a2-42d14c99645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3.Using both dataframes from Part 1 and Part 2, generate a report that will provide the value for series_id = PRS30006032 and period = Q01 and the population for that given year (if available in the population dataset). The below table shows an example of one row that might appear in the resulting table:\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_filtered_bls = (\n",
    "    df_bls\n",
    "    .filter(\n",
    "        (col(\"series_id\") == \"PRS30006032\") &\n",
    "        (col(\"period\") == \"Q01\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "101d514a-514a-4dbf-a219-2b3b8c57f5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------+-----+--------------+\n",
      "|  series_id|year|period|value|footnote_codes|\n",
      "+-----------+----+------+-----+--------------+\n",
      "|PRS30006032|1988|   Q01|  2.1|          NULL|\n",
      "|PRS30006032|1989|   Q01|  1.8|          NULL|\n",
      "|PRS30006032|1990|   Q01| -4.6|          NULL|\n",
      "|PRS30006032|1991|   Q01| -7.9|          NULL|\n",
      "|PRS30006032|1992|   Q01| -3.1|          NULL|\n",
      "|PRS30006032|1993|   Q01|  1.3|          NULL|\n",
      "|PRS30006032|1994|   Q01|  1.7|          NULL|\n",
      "|PRS30006032|1995|   Q01|  0.0|          NULL|\n",
      "|PRS30006032|1996|   Q01| -4.2|          NULL|\n",
      "|PRS30006032|1997|   Q01|  2.8|          NULL|\n",
      "|PRS30006032|1998|   Q01|  0.9|          NULL|\n",
      "|PRS30006032|1999|   Q01| -4.1|          NULL|\n",
      "|PRS30006032|2000|   Q01|  0.5|          NULL|\n",
      "|PRS30006032|2001|   Q01| -6.3|          NULL|\n",
      "|PRS30006032|2002|   Q01| -6.6|          NULL|\n",
      "|PRS30006032|2003|   Q01| -5.7|          NULL|\n",
      "|PRS30006032|2004|   Q01|  2.0|          NULL|\n",
      "|PRS30006032|2005|   Q01| -0.5|          NULL|\n",
      "|PRS30006032|2006|   Q01|  1.8|          NULL|\n",
      "|PRS30006032|2007|   Q01| -0.8|          NULL|\n",
      "+-----------+----+------+-----+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_filtered_bls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ecd6030a-dd63-450b-81aa-e7959b081ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population_clean = (\n",
    "    df_population\n",
    "    .withColumnRenamed(\"Year\", \"year\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "de6388d3-e62a-44b5-bd54-cac441883071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report = (\n",
    "    df_filtered_bls\n",
    "    .join(\n",
    "        df_population_clean,\n",
    "        on=\"year\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .select(\n",
    "        \"series_id\",\n",
    "        \"year\",\n",
    "        \"period\",\n",
    "        \"value\",\n",
    "        \"population\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff5f280f-471f-495c-bee9-eea0384ca1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------+-----+----------+\n",
      "|  series_id|year|period|value|Population|\n",
      "+-----------+----+------+-----+----------+\n",
      "|PRS30006032|1988|   Q01|  2.1|      NULL|\n",
      "|PRS30006032|1989|   Q01|  1.8|      NULL|\n",
      "|PRS30006032|1990|   Q01| -4.6|      NULL|\n",
      "|PRS30006032|1991|   Q01| -7.9|      NULL|\n",
      "|PRS30006032|1992|   Q01| -3.1|      NULL|\n",
      "|PRS30006032|1993|   Q01|  1.3|      NULL|\n",
      "|PRS30006032|1994|   Q01|  1.7|      NULL|\n",
      "|PRS30006032|1995|   Q01|  0.0|      NULL|\n",
      "|PRS30006032|1996|   Q01| -4.2|      NULL|\n",
      "|PRS30006032|1997|   Q01|  2.8|      NULL|\n",
      "|PRS30006032|1998|   Q01|  0.9|      NULL|\n",
      "|PRS30006032|1999|   Q01| -4.1|      NULL|\n",
      "|PRS30006032|2000|   Q01|  0.5|      NULL|\n",
      "|PRS30006032|2001|   Q01| -6.3|      NULL|\n",
      "|PRS30006032|2002|   Q01| -6.6|      NULL|\n",
      "|PRS30006032|2003|   Q01| -5.7|      NULL|\n",
      "|PRS30006032|2004|   Q01|  2.0|      NULL|\n",
      "|PRS30006032|2005|   Q01| -0.5|      NULL|\n",
      "|PRS30006032|2006|   Q01|  1.8|      NULL|\n",
      "|PRS30006032|2007|   Q01| -0.8|      NULL|\n",
      "+-----------+----+------+-----+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_report.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "69231512-3e24-47ad-a0a2-f3b72fdf15b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------+-----+----------+\n",
      "|  series_id|year|period|value|population|\n",
      "+-----------+----+------+-----+----------+\n",
      "|PRS30006032|2013|   Q01|  0.5| 316128839|\n",
      "|PRS30006032|2014|   Q01| -0.1| 318857056|\n",
      "|PRS30006032|2015|   Q01| -1.7| 321418821|\n",
      "|PRS30006032|2016|   Q01| -1.4| 323127515|\n",
      "|PRS30006032|2017|   Q01|  0.9| 325719178|\n",
      "|PRS30006032|2018|   Q01|  0.5| 327167439|\n",
      "|PRS30006032|2019|   Q01| -1.6| 328239523|\n",
      "|PRS30006032|2021|   Q01|  0.7| 331893745|\n",
      "|PRS30006032|2022|   Q01|  5.3| 333287562|\n",
      "|PRS30006032|2023|   Q01|  0.3| 334914896|\n",
      "+-----------+----+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_report_not_null = df_report.filter(col(\"population\").isNotNull())\n",
    "\n",
    "df_report_not_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a9cca-bb2f-4ecd-8bcc-5a02573eae62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
